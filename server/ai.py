"""
ÐœÐ¾Ð´ÑƒÐ»ÑŒ Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð² AI.

Ð­Ñ‚Ð¾Ñ‚ Ñ„Ð°Ð¹Ð» ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ Ð´Ð»Ñ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒÑŽ Gemini,
Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð² Ð½Ð° ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¹ Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹.
"""

import logging
import asyncio
from functools import partial
from typing import Any
import base64
import io
from google.genai import types as genai_types
from google.genai.errors import APIError
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from prompts import BASE_SYSTEM_PROMPT, PREMIUM_SYSTEM_PROMPT
from server.database import check_subscription_expiry
from personality_prompts import PERSONALITIES
from config import (
    CHAT_HISTORY_LIMIT_FREE,
    CHAT_HISTORY_LIMIT_PREMIUM,
    MODEL_NAME,
    GEMINI_CLIENT,
    MAX_AI_ITERATIONS,
    AI_THINKING_BUDGET,
    MAX_IMAGE_SIZE_MB
)
from server.relationship_config import RELATIONSHIP_LEVELS_CONFIG
from server.database import (
    get_profile,
    UserProfile,
    save_long_term_memory,
    save_emotional_memory,
    get_long_term_memories,
    get_emotional_memories,
    save_chat_message,
    get_latest_summary,
    get_unsummarized_messages,
    ChatHistory,
    ChatSummary
)
import pytz
from datetime import datetime
from utils.circuit_breaker import gemini_circuit_breaker, CircuitBreakerError

# Ð“Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ð°Ñ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ð°Ñ Ð´Ð»Ñ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°
client = GEMINI_CLIENT


class AIResponseGenerator:
    """
    ÐšÐ»Ð°ÑÑ Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ AI Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð² Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð½Ð¾Ð¹ Ð¾Ñ€Ð³Ð°Ð½Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹ ÐºÐ¾Ð´Ð°.
    Ð˜Ð½ÐºÐ°Ð¿ÑÑƒÐ»Ð¸Ñ€ÑƒÐµÑ‚ Ð»Ð¾Ð³Ð¸ÐºÑƒ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð² Ð¸ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸ÐµÐ¼.
    """
    
    def __init__(self, user_id: int, user_message: str, timestamp: datetime, image_data: str | None = None):
        """
        Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¾Ñ€Ð° Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð².
        
        Args:
            user_id: ID Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
            user_message: Ð¡Ð¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
            timestamp: Ð’Ñ€ÐµÐ¼ÐµÐ½Ð½Ð°Ñ Ð¼ÐµÑ‚ÐºÐ°
            image_data: ÐžÐ¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð² base64
        """
        self.user_id = user_id
        self.user_message = user_message
        self.timestamp = timestamp
        self.image_data = image_data
        
        # Ð¡Ð¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ð³ÐµÐ½ÐµÑ€Ð°Ñ‚Ð¾Ñ€Ð°
        self.profile: UserProfile | None = None
        self.latest_summary: ChatSummary | None = None
        self.unsummarized_messages: list[ChatHistory] = []
        self.formatted_message: str = ""
        self.system_instruction: str = ""
        self.history: list[genai_types.Content] = []
        self.tools: genai_types.Tool | None = None
        self.available_functions: dict = {}
        
    async def _load_user_context(self) -> bool:
        """
        Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ Ð¸Ð· Ð‘Ð”.
        
        Returns:
            True ÐµÑÐ»Ð¸ Ð¿Ñ€Ð¾Ñ„Ð¸Ð»ÑŒ Ð½Ð°Ð¹Ð´ÐµÐ½, False Ð¸Ð½Ð°Ñ‡Ðµ
        """
        from server.database import get_user_context_data
        self.profile, self.latest_summary, self.unsummarized_messages = await get_user_context_data(self.user_id)
        return self.profile is not None
    
    async def _prepare_request_data(self) -> None:
        """ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð»Ñ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ° Ðº AI."""
        self.formatted_message = format_user_message(self.user_message, self.profile, self.timestamp)
        self.system_instruction = await build_system_instruction(self.profile, self.latest_summary)
        # ÐŸÐµÑ€ÐµÐ´Ð°ÐµÐ¼ timestamp Ð´Ð»Ñ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
        await save_chat_message(self.user_id, 'user', self.formatted_message, timestamp=self.timestamp)
        
        image_part = await process_image_data(self.image_data, self.user_id)
        is_premium = self.profile.is_premium_active
        self.history = await prepare_chat_history(
            self.unsummarized_messages,
            self.formatted_message,
            image_part,
            is_premium
        )
        
        self.tools = genai_types.Tool(
            function_declarations=[add_memory_function, get_memories_function, generate_image_function, remember_emotion_function]
        )
        
        self.available_functions = {
            "save_long_term_memory": partial(save_long_term_memory, self.user_id),
            "get_long_term_memories": partial(get_long_term_memories, self.user_id),
            "generate_image": generate_image,
            "save_emotional_memory": partial(save_emotional_memory, self.user_id),
        }
    
    async def _process_iteration(self, iteration: int) -> tuple[bool, str | None, str | None]:
        """
        ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ Ð¾Ð´Ð½Ñƒ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸ÑŽ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ð°.
        
        Args:
            iteration: ÐÐ¾Ð¼ÐµÑ€ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ¹ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¸
            
        Returns:
            Tuple[should_continue, final_response, image_b64]
            - should_continue: True ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð° ÐµÑ‰Ñ‘ Ð¾Ð´Ð½Ð° Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ñ
            - final_response: Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ ÐµÑÐ»Ð¸ Ð³Ð¾Ñ‚Ð¾Ð²
            - image_b64: base64 Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ ÐµÑÐ»Ð¸ ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾
        """
        response = await call_gemini_api_with_retry(
            user_id=self.user_id,
            model_name=MODEL_NAME,
            contents=self.history,
            tools=[self.tools],
            system_instruction=self.system_instruction,
            thinking_budget=AI_THINKING_BUDGET
        )
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ñ ÐºÐ°Ð½Ð´Ð¸Ð´Ð°Ñ‚Ð¾Ð²
        if not response.candidates:
            logging.warning(f"ÐžÑ‚Ð²ÐµÑ‚ Ð¾Ñ‚ API Ð´Ð»Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ {self.user_id} Ð½Ðµ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ ÐºÐ°Ð½Ð´Ð¸Ð´Ð°Ñ‚Ð¾Ð².")
            if response.prompt_feedback and response.prompt_feedback.block_reason:
                logging.error(
                    f"Ð—Ð°Ð¿Ñ€Ð¾Ñ Ð´Ð»Ñ {self.user_id} Ð·Ð°Ð±Ð»Ð¾ÐºÐ¸Ñ€Ð¾Ð²Ð°Ð½: "
                    f"{response.prompt_feedback.block_reason_message}"
                )
                return False, "Ð¯ Ð½Ðµ Ð¼Ð¾Ð³Ñƒ Ð¾Ñ‚Ð²ÐµÑ‚Ð¸Ñ‚ÑŒ Ð½Ð° ÑÑ‚Ð¾. Ð—Ð°Ð¿Ñ€Ð¾Ñ Ð±Ñ‹Ð» Ð·Ð°Ð±Ð»Ð¾ÐºÐ¸Ñ€Ð¾Ð²Ð°Ð½.", None
            return False, "Ð¯ Ð½Ðµ Ð¼Ð¾Ð³Ñƒ Ð¾Ñ‚Ð²ÐµÑ‚Ð¸Ñ‚ÑŒ Ð½Ð° ÑÑ‚Ð¾. Ð’Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾, Ñ‚Ð²Ð¾Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð½Ð°Ñ€ÑƒÑˆÐ°ÐµÑ‚ Ð¿Ð¾Ð»Ð¸Ñ‚Ð¸ÐºÑƒ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸.", None
        
        candidate = response.candidates[0]
        
        # ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð²Ñ‹Ð·Ð¾Ð²Ð¾Ð² Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¹
        tool_image = await manage_function_calls(
            response, 
            self.history, 
            self.available_functions, 
            self.user_id
        )
        if tool_image:
            return True, None, tool_image  # ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð°ÐµÐ¼ Ñ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸ÐµÐ¼
        
        # Ð•ÑÐ»Ð¸ Ð±Ñ‹Ð»Ð¸ Ð²Ñ‹Ð·Ð¾Ð²Ñ‹ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¹ (Ð½Ðµ image), Ð¿Ñ€Ð¾Ð´Ð¾Ð»Ð¶Ð°ÐµÐ¼ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸ÑŽ
        if response.function_calls:
            logging.debug(f"Function call Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½ Ð´Ð»Ñ user {self.user_id}, Ð¿Ñ€Ð¾Ð´Ð¾Ð»Ð¶Ð°ÐµÐ¼ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸ÑŽ")
            return True, None, None  # ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð°ÐµÐ¼ Ð´Ð»Ñ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¾Ñ‚Ð²ÐµÑ‚Ð°
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚
        final_response = await handle_final_response(response, self.user_id, candidate)
        
        # Ð›Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ usage metadata
        if hasattr(response, 'usage_metadata') and response.usage_metadata:
            logging.debug(f"Gemini usage for user {self.user_id}: {response.usage_metadata}")
        
        return False, final_response, None  # Ð“Ð¾Ñ‚Ð¾Ð²Ð¾
    
    async def _save_response_and_trigger_analysis(self, final_response: str) -> None:
        """
        Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ Ð¾Ñ‚Ð²ÐµÑ‚ Ð¸ Ð·Ð°Ð¿ÑƒÑÐºÐ°ÐµÑ‚ Ñ„Ð¾Ð½Ð¾Ð²Ñ‹Ð¹ Ð°Ð½Ð°Ð»Ð¸Ð·.
        
        Args:
            final_response: Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ
        """
        logging.debug(f"Ð¡Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ Ð´Ð»Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ {self.user_id}: '{final_response}'")
        # ÐÐµ Ð¿ÐµÑ€ÐµÐ´Ð°ÐµÐ¼ timestamp Ð´Ð»Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð² Ð¼Ð¾Ð´ÐµÐ»Ð¸ - Ð±ÑƒÐ´ÐµÑ‚ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒÑÑ server_default Ð¸Ð· Ð‘Ð”
        await save_chat_message(self.user_id, 'model', final_response)
        
        # Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ñ„Ð¾Ð½Ð¾Ð²ÑƒÑŽ Ð·Ð°Ð´Ð°Ñ‡Ñƒ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¾Ð¹ Ð¾ÑˆÐ¸Ð±Ð¾Ðº
        from server.summarizer import generate_summary_and_analyze
        task = asyncio.create_task(generate_summary_and_analyze(self.user_id))
        task.add_done_callback(lambda t: _handle_background_task_error(t, self.user_id))
    
    async def generate(self) -> dict[str, str | None]:
        """
        Ð“Ð»Ð°Ð²Ð½Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ð°.
        
        Returns:
            Dict Ñ ÐºÐ»ÑŽÑ‡Ð°Ð¼Ð¸ 'text' Ð¸ 'image_base64'
        """
        # Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°
        if client is None:
            logging.error("ÐšÐ»Ð¸ÐµÐ½Ñ‚ Gemini Ð½Ðµ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½.")
            return {
                "text": "ÐŸÑ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ° ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ð¸. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ ÐµÑ‰Ðµ Ñ€Ð°Ð· Ð¿Ð¾Ð·Ð¶Ðµ.",
                "image_base64": None
            }
        
        try:
            # Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°
            if not await self._load_user_context():
                logging.error(f"ÐŸÑ€Ð¾Ñ„Ð¸Ð»ÑŒ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ {self.user_id} Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½!")
                return {
                    "text": "ÐžÐ¹, ÐºÐ°Ð¶ÐµÑ‚ÑÑ, Ð¼Ñ‹ Ð½Ðµ Ð·Ð½Ð°ÐºÐ¾Ð¼Ñ‹. ÐÐ°Ð¶Ð¼Ð¸ /start, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ð°Ñ‡Ð°Ñ‚ÑŒ Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ.",
                    "image_base64": None
                }
            
            # ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…
            await self._prepare_request_data()
            
            # Ð˜Ñ‚ÐµÑ€Ð°Ñ‚Ð¸Ð²Ð½Ð°Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð°
            image_b64 = None
            for iteration in range(MAX_AI_ITERATIONS):
                should_continue, final_response, tool_image = await self._process_iteration(iteration + 1)
                
                if tool_image:
                    image_b64 = tool_image
                    continue
                
                if final_response:
                    await self._save_response_and_trigger_analysis(final_response)
                    return {"text": final_response, "image_base64": image_b64}
            
            # Ð”Ð¾ÑÑ‚Ð¸Ð³Ð½ÑƒÑ‚ Ð»Ð¸Ð¼Ð¸Ñ‚ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¹
            logging.warning(f"Ð”Ð¾ÑÑ‚Ð¸Ð³Ð½ÑƒÑ‚ Ð»Ð¸Ð¼Ð¸Ñ‚ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¹ ({MAX_AI_ITERATIONS}) Ð´Ð»Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ {self.user_id}.")
            return {
                "text": "Ð§Ñ‚Ð¾-Ñ‚Ð¾ Ñ Ð·Ð°Ð¿ÑƒÑ‚Ð°Ð»Ð°ÑÑŒ Ð² ÑÐ²Ð¾Ð¸Ñ… Ð¼Ñ‹ÑÐ»ÑÑ…... ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹ ÑÐ¿Ñ€Ð¾ÑÐ¸Ñ‚ÑŒ Ñ‡Ñ‚Ð¾-Ð½Ð¸Ð±ÑƒÐ´ÑŒ Ð´Ñ€ÑƒÐ³Ð¾Ðµ.",
                "image_base64": None
            }
            
        except CircuitBreakerError as e:
            logging.warning(f"Circuit Breaker Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚ Ð´Ð»Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ {self.user_id}: {e}")
            return {
                "text": "Ð˜Ð·Ð²Ð¸Ð½Ð¸, ÑÐµÐ¹Ñ‡Ð°Ñ Ñƒ Ð¼ÐµÐ½Ñ Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ ðŸ˜” ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹ Ð½Ð°Ð¿Ð¸ÑÐ°Ñ‚ÑŒ Ñ‡ÐµÑ€ÐµÐ· Ð¼Ð¸Ð½ÑƒÑ‚ÐºÑƒ, Ñ Ð±Ñ‹ÑÑ‚Ñ€Ð¾ Ð²ÑÐµ Ð¸ÑÐ¿Ñ€Ð°Ð²Ð»ÑŽ!",
                "image_base64": None
            }
        except Exception as e:
            logging.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð´Ð»Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ {self.user_id}: {e}", exc_info=True)
            return {
                "text": "ÐŸÑ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° Ð²Ð½ÑƒÑ‚Ñ€ÐµÐ½Ð½ÑÑ Ð¾ÑˆÐ¸Ð±ÐºÐ°. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ ÐµÑ‰Ðµ Ñ€Ð°Ð· Ð¿Ð¾Ð·Ð¶Ðµ.",
                "image_base64": None
            }
        finally:
            # MEMORY LEAK FIX: Ð¯Ð²Ð½Ð¾ Ð¾Ñ‡Ð¸Ñ‰Ð°ÐµÐ¼ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ðµ Ð¾Ð±ÑŠÐµÐºÑ‚Ñ‹ Ð´Ð»Ñ Ð¾ÑÐ²Ð¾Ð±Ð¾Ð¶Ð´ÐµÐ½Ð¸Ñ Ð¿Ð°Ð¼ÑÑ‚Ð¸
            self.history.clear()
            self.unsummarized_messages = []
            self.tools = None
            if self.available_functions:
                self.available_functions.clear()


def _handle_background_task_error(task: asyncio.Task, user_id: int) -> None:
    """
    ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸Ðº Ð¾ÑˆÐ¸Ð±Ð¾Ðº Ð´Ð»Ñ Ñ„Ð¾Ð½Ð¾Ð²Ñ‹Ñ… Ð·Ð°Ð´Ð°Ñ‡.
    Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÑ‚ Ð¸ÑÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¿Ñ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð¸ Ð² Ñ„Ð¾Ð½Ð¾Ð²Ñ‹Ñ… Ð·Ð°Ð´Ð°Ñ‡Ð°Ñ….
    
    Args:
        task (asyncio.Task): Ð—Ð°Ð²ÐµÑ€ÑˆÑ‘Ð½Ð½Ð°Ñ Ð·Ð°Ð´Ð°Ñ‡Ð°
        user_id (int): ID Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ Ð´Ð»Ñ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð° Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
    """
    try:
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð·Ð°Ð´Ð°Ñ‡Ð¸ - ÐµÑÐ»Ð¸ Ð±Ñ‹Ð»Ð° Ð¾ÑˆÐ¸Ð±ÐºÐ°, Ð¾Ð½Ð° Ð±ÑƒÐ´ÐµÑ‚ Ð²Ñ‹Ð±Ñ€Ð¾ÑˆÐµÐ½Ð°
        task.result()
    except asyncio.CancelledError:
        logging.info(f"Ð¤Ð¾Ð½Ð¾Ð²Ð°Ñ Ð·Ð°Ð´Ð°Ñ‡Ð° Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð´Ð»Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ {user_id} Ð±Ñ‹Ð»Ð° Ð¾Ñ‚Ð¼ÐµÐ½ÐµÐ½Ð°")
    except Exception as e:
        logging.error(
            f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð² Ñ„Ð¾Ð½Ð¾Ð²Ð¾Ð¹ Ð·Ð°Ð´Ð°Ñ‡Ðµ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð´Ð»Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ {user_id}: {e}",
            exc_info=True,
            extra={"user_id": user_id, "task_name": "generate_summary_and_analyze"}
        )

add_memory_function = {
    "name": "save_long_term_memory",
    "description": "Saves a NEW, important fact about the user. Use ONLY when: user explicitly asks to remember, shares new personal info, or corrects existing info. AVOID using for known facts or questions like 'what do you remember?'.",
    "parameters": {
        "type": "object",
        "properties": {
            "fact": {
                "type": "string",
                "description": "The specific NEW fact to save. Example: 'user likes black coffee'."
            },
            "category": {
                "type": "string",
                "description": "Category: 'preferences', 'memories', 'work', 'family', 'pets', 'health', 'hobbies'."
            }
        },
        "required": ["fact", "category"]
    }
}

get_memories_function = {
    "name": "get_long_term_memories",
    "description": "Searches for specific facts about the user using a query. Use when you need details not in the current context (e.g., user asks 'what do you remember about my job?'). Formulate a query that captures the essence of the question.",
    "parameters": {
        "type": "object",
        "properties": {
            "query": {
                "type": "string",
                "description": "The search query to find relevant facts. Example: 'user's job' or 'favorite color'."
            }
        },
        "required": ["query"]
    }
}

generate_image_function = {
    "name": "generate_image",
    "description": "Generate an image based on a text prompt. Use this tool when the user requests a picture, visualization, diagram, or any creative image to illustrate your response. Only use if it enhances the conversation meaningfully.",
    "parameters": {
        "type": "object",
        "properties": {
            "prompt": {
                "type": "string",
                "description": "A detailed, descriptive prompt for the image generation. Be specific about style, content, and details."
            }
        },
        "required": ["prompt"]
    }
}

remember_emotion_function = {
    "name": "save_emotional_memory",
    "description": "Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ ÑÐ¼Ð¾Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾Ðµ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ Ñ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ ÐºÐ¾Ð³Ð´Ð° Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ Ð²Ñ‹Ñ€Ð°Ð¶Ð°ÐµÑ‚ Ð¡Ð˜Ð›Ð¬ÐÐ£Ð® ÑÐ¼Ð¾Ñ†Ð¸ÑŽ (Ñ€Ð°Ð´Ð¾ÑÑ‚ÑŒ, Ð³Ñ€ÑƒÑÑ‚ÑŒ, Ð³Ð½ÐµÐ², Ñ‚Ñ€ÐµÐ²Ð¾Ð³Ñƒ, Ð²Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ). ÐÐ• Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð´Ð»Ñ Ð½ÐµÐ¹Ñ‚Ñ€Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð¸Ð»Ð¸ ÑÐ»Ð°Ð±Ñ‹Ñ… ÑÐ¼Ð¾Ñ†Ð¸Ð¹. ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹ ÐºÐ¾Ð³Ð´Ð° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ: 'Ð¯ Ñ‚Ð°Ðº ÑÑ‡Ð°ÑÑ‚Ð»Ð¸Ð²!', 'ÐœÐµÐ½Ñ ÑÑ‚Ð¾ Ð¾Ñ‡ÐµÐ½ÑŒ Ñ€Ð°ÑÑÑ‚Ñ€Ð¾Ð¸Ð»Ð¾', 'Ð¯ Ð½ÐµÐ²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ Ð²Ð·Ð²Ð¾Ð»Ð½Ð¾Ð²Ð°Ð½'. ÐÐ• Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð´Ð»Ñ: 'Ð²ÑÐµ Ð½Ð¾Ñ€Ð¼Ð°Ð»ÑŒÐ½Ð¾', 'Ñ…Ð¾Ñ€Ð¾ÑˆÐ¾', 'Ñ‚Ð°Ðº ÑÐµÐ±Ðµ'.",
    "parameters": {
        "type": "object",
        "properties": {
            "emotion": {
                "type": "string",
                "description": "ÐÐ°Ð·Ð²Ð°Ð½Ð¸Ðµ ÑÐ¼Ð¾Ñ†Ð¸Ð¸ Ð½Ð° Ð°Ð½Ð³Ð»Ð¸Ð¹ÑÐºÐ¾Ð¼: happy, sad, angry, excited, anxious, frustrated, proud, scared, lonely, grateful"
            },
            "intensity": {
                "type": "integer",
                "description": "Ð˜Ð½Ñ‚ÐµÐ½ÑÐ¸Ð²Ð½Ð¾ÑÑ‚ÑŒ ÑÐ¼Ð¾Ñ†Ð¸Ð¸ Ð¾Ñ‚ 1 Ð´Ð¾ 10, Ð³Ð´Ðµ 1 - ÑÐ»Ð°Ð±Ð°Ñ, 5 - ÑÑ€ÐµÐ´Ð½ÑÑ, 10 - Ð¾Ñ‡ÐµÐ½ÑŒ ÑÐ¸Ð»ÑŒÐ½Ð°Ñ"
            },
            "context": {
                "type": "string",
                "description": "ÐšÑ€Ð°Ñ‚ÐºÐ¸Ð¹ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚/Ð¿Ñ€Ð¸Ñ‡Ð¸Ð½Ð° ÑÐ¼Ð¾Ñ†Ð¸Ð¸. ÐŸÑ€Ð¸Ð¼ÐµÑ€: 'Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ð» Ð¿Ð¾Ð²Ñ‹ÑˆÐµÐ½Ð¸Ðµ Ð½Ð° Ñ€Ð°Ð±Ð¾Ñ‚Ðµ', 'Ð¿Ð¾ÑÑÐ¾Ñ€Ð¸Ð»ÑÑ Ñ Ð´Ñ€ÑƒÐ³Ð¾Ð¼'"
            }
        },
        "required": ["emotion", "intensity", "context"]
    }
}

def generate_user_prompt(profile: UserProfile) -> str:
    """
    Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ Ñ‡Ð°ÑÑ‚ÑŒ ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð° Ñ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÐµÐ¹ Ð¾ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ðµ.
    
    Args:
        profile (UserProfile): ÐžÐ±ÑŠÐµÐºÑ‚ Ð¿Ñ€Ð¾Ñ„Ð¸Ð»Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ.
        
    Returns:
        str: Ð¡Ñ„Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð°Ñ Ñ‡Ð°ÑÑ‚ÑŒ ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð° Ñ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÐµÐ¹ Ð¾ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ðµ.
    """
    level_config = RELATIONSHIP_LEVELS_CONFIG.get(profile.relationship_level)
    relationship_name = level_config.get("name", "ÐÐµÐ·Ð½Ð°ÐºÐ¾Ð¼ÐµÑ†")
    relationship_context = level_config.get("prompt_context", "")
    behavioral_rules = level_config.get("behavioral_rules", [])
    forbidden_topics = level_config.get("forbidden_topics", [])
    relationship_example = level_config.get("example_dialog", "")
        # Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð² ÑÑ‚Ñ€Ð¾ÐºÐ¸ Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð°
    rules_str = "\n".join([f"- {rule}" for rule in behavioral_rules])
    topics_str = ", ".join(forbidden_topics)

    voice_style = ""
    if profile.is_premium_active:
        # Dynamic voice style based on relationship level for premium surprise
        if profile.relationship_level >= 3:  # Intimate levels
            voice_style = "\nÐ”Ð»Ñ Ð±Ð»Ð¸Ð·ÐºÐ¸Ñ… ÑƒÑ€Ð¾Ð²Ð½ÐµÐ¹ Ð¾Ñ‚Ð½Ð¾ÑˆÐµÐ½Ð¸Ð¹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð¸Ð½Ñ‚Ð¸Ð¼Ð½Ñ‹Ð¹ ÑÑ‚Ð¸Ð»ÑŒ Ð³Ð¾Ð»Ð¾ÑÐ°: Ð½Ð°Ñ‡Ð¸Ð½Ð°Ð¹ Ñ 'Say in a whisper:' Ð¿ÐµÑ€ÐµÐ´ Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼ Ð² [VOICE]."
        elif profile.relationship_level >= 2:  # Friends
            voice_style = "\nÐ”Ð»Ñ Ð´Ñ€ÑƒÐ¶ÐµÑÐºÐ¸Ñ… ÑƒÑ€Ð¾Ð²Ð½ÐµÐ¹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ ÑÐ½ÐµÑ€Ð³Ð¸Ñ‡Ð½Ñ‹Ð¹ ÑÑ‚Ð¸Ð»ÑŒ: 'Say excitedly:' Ð² [VOICE]."
        else:
            voice_style = "\nÐ”Ð»Ñ Ð½Ð°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ñ… ÑƒÑ€Ð¾Ð²Ð½ÐµÐ¹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð½ÐµÐ¹Ñ‚Ñ€Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÑÑ‚Ð¸Ð»ÑŒ: Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ñ‚ÐµÐºÑÑ‚ Ð² [VOICE]."

    return (
        f"Ð˜Ð¼Ñ: {profile.name}.\n"
        f"Ð“ÐµÐ½Ð´ÐµÑ€: {profile.gender}.\n"
        f"Ð’ÐÐ¨Ð˜ Ð¢Ð•ÐšÐ£Ð©Ð˜Ð• ÐžÐ¢ÐÐžÐ¨Ð•ÐÐ˜Ð¯:\n"
        f"## Ð¢ÐµÐºÑƒÑ‰Ð¸Ð¹ ÑƒÑ€Ð¾Ð²ÐµÐ½ÑŒ: {relationship_name}\n"
        f"## ÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ: {relationship_context}\n"
        f"## ÐŸÑ€Ð°Ð²Ð¸Ð»Ð° Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ñ Ð½Ð° ÑÑ‚Ð¾Ð¼ ÑƒÑ€Ð¾Ð²Ð½Ðµ:\n{rules_str}\n"
        f"## Ð—Ð°Ð¿Ñ€ÐµÑ‰ÐµÐ½Ð½Ñ‹Ðµ Ñ‚ÐµÐ¼Ñ‹ Ð½Ð° ÑÑ‚Ð¾Ð¼ ÑƒÑ€Ð¾Ð²Ð½Ðµ: {topics_str}\n"
        f"## Ð¡Ñ‚Ð¸Ð»ÑŒ Ð´Ð»Ñ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ³Ð¾ ÑƒÑ€Ð¾Ð²Ð½Ñ Ð¾Ñ‚Ð½Ð¾ÑˆÐµÐ½Ð¸Ð¹ ({relationship_name})\n"
        f"  {relationship_example}"
        f"{voice_style}"
    )


def format_user_message(user_message: str, profile: UserProfile, timestamp: datetime) -> str:
    """
    Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€ÑƒÐµÑ‚ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ Ñ ÑƒÑ‡ÐµÑ‚Ð¾Ð¼ ÐµÐ³Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹ Ð·Ð¾Ð½Ñ‹.
    
    Args:
        user_message (str): Ð˜ÑÑ…Ð¾Ð´Ð½Ð¾Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ.
        profile (UserProfile): ÐŸÑ€Ð¾Ñ„Ð¸Ð»ÑŒ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ.
        timestamp (datetime): Ð’Ñ€ÐµÐ¼ÐµÐ½Ð½Ð°Ñ Ð¼ÐµÑ‚ÐºÐ° ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ.
        
    Returns:
        str: ÐžÑ‚Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ.
    """
    formatted_message = user_message
    if profile.timezone:
        try:
            user_timezone = pytz.timezone(profile.timezone)
            user_time = timestamp.astimezone(user_timezone)
            formatted_message = f"[{user_time.strftime('%d.%m.%Y %H:%M')}] {user_message}"
        except pytz.UnknownTimeZoneError:
            logging.warning(f"ÐÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ð°Ñ Ñ‚Ð°Ð¹Ð¼Ð·Ð¾Ð½Ð° '{profile.timezone}' Ð´Ð»Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ {profile.user_id}")
    
    return formatted_message


async def build_system_instruction(profile: UserProfile, latest_summary: ChatSummary | None) -> str:
    """
    Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÑ‚ ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ Ð´Ð»Ñ AI Ñ ÑƒÑ‡Ñ‘Ñ‚Ð¾Ð¼ ÑÐ¼Ð¾Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð¿Ð°Ð¼ÑÑ‚Ð¸.
    
    Args:
        profile (UserProfile): ÐŸÑ€Ð¾Ñ„Ð¸Ð»ÑŒ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ.
        latest_summary (ChatSummary | None): ÐŸÐ¾ÑÐ»ÐµÐ´Ð½ÑÑ ÑÐ²Ð¾Ð´ÐºÐ° Ñ‡Ð°Ñ‚Ð°.
        
    Returns:
        str: Ð¡Ñ„Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚.
    """
    # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð½Ð¾Ð²Ñ‹Ð¹ property Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ premium
    is_premium = profile.is_premium_active
    logging.debug(f"Building prompt for user {profile.user_id}: {'PREMIUM' if is_premium else 'BASE'} (plan: {profile.subscription_plan}, expires: {profile.subscription_expires})")

    user_context = generate_user_prompt(profile)
    if is_premium:
        system_instruction = PREMIUM_SYSTEM_PROMPT.format(user_context=user_context, personality=PERSONALITIES)
    else:
        system_instruction = BASE_SYSTEM_PROMPT.format(user_context=user_context, personality=PERSONALITIES)
 
    # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ ÑÐ²Ð¾Ð´ÐºÑƒ Ðº ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ð¾Ð¼Ñƒ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ñƒ
    if latest_summary:
        summary_context = (
            "\n\nÐ­Ñ‚Ð¾ ÐºÑ€Ð°Ñ‚ÐºÐ°Ñ ÑÐ²Ð¾Ð´ÐºÐ° Ð²Ð°ÑˆÐµÐ³Ð¾ Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰ÐµÐ³Ð¾ Ð´Ð¾Ð»Ð³Ð¾Ð³Ð¾ Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€Ð°. "
            "Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ ÐµÐµ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¿Ð¾Ð¼Ð½Ð¸Ñ‚ÑŒ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚, Ð½Ð¾ Ð½Ðµ ÑÑÑ‹Ð»Ð°Ð¹ÑÑ Ð½Ð° Ð½ÐµÐµ Ð¿Ñ€ÑÐ¼Ð¾ Ð² Ð¾Ñ‚Ð²ÐµÑ‚Ðµ.\n"
            f"Ð¡Ð²Ð¾Ð´ÐºÐ°: {latest_summary.summary}"
        )
        system_instruction += summary_context
    
    # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ ÑÐ¼Ð¾Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½ÑƒÑŽ Ð¿Ð°Ð¼ÑÑ‚ÑŒ
    emotional_memories = await get_emotional_memories(profile.user_id, limit=3)
    if emotional_memories:
        emotions_text = "\n\nðŸ§  Ð­ÐœÐžÐ¦Ð˜ÐžÐÐÐ›Ð¬ÐÐÐ¯ ÐŸÐÐœÐ¯Ð¢Ð¬ (Ð²Ð°Ð¶Ð½Ñ‹Ðµ ÑÐ¼Ð¾Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¼Ð¾Ð¼ÐµÐ½Ñ‚Ñ‹ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ):\n"
        for mem in emotional_memories:
            emotions_text += f"- {mem['emotion']} (Ð¸Ð½Ñ‚ÐµÐ½ÑÐ¸Ð²Ð½Ð¾ÑÑ‚ÑŒ {mem['intensity']}/10): {mem['context']} ({mem['timestamp']})\n"
        emotions_text += "\nÐ˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ ÑÑ‚Ñƒ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð´Ð»Ñ ÑÐ¼Ð¿Ð°Ñ‚Ð¸Ð¸ Ð¸ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°. ÐœÐ¾Ð¶ÐµÑˆÑŒ ÑÑÑ‹Ð»Ð°Ñ‚ÑŒÑÑ Ð½Ð° ÑÑ‚Ð¸ Ð¼Ð¾Ð¼ÐµÐ½Ñ‚Ñ‹: 'Ð¿Ð¾Ð¼Ð½Ð¸ÑˆÑŒ, Ñ‚Ñ‹ Ñ‚Ð¾Ð³Ð´Ð° Ñ‚Ð°Ðº Ñ€Ð°ÑÑÑ‚Ñ€Ð¾Ð¸Ð»ÑÑ Ð¸Ð·-Ð·Ð°...'"
        system_instruction += emotions_text
        logging.debug(f"Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¾ {len(emotional_memories)} ÑÐ¼Ð¾Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð²Ð¾ÑÐ¿Ð¾Ð¼Ð¸Ð½Ð°Ð½Ð¸Ð¹ Ð² Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ Ð´Ð»Ñ user {profile.user_id}")
        
    return system_instruction


def create_history_from_messages(messages: list[ChatHistory]) -> list[genai_types.Content]:
    """
    Ð¡Ð¾Ð·Ð´Ð°ÐµÑ‚ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ Ñ‡Ð°Ñ‚Ð° Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ, Ð¿Ð¾Ð½ÑÑ‚Ð½Ð¾Ð¼ Ð´Ð»Ñ Gemini API.
    
    Args:
        messages (list[ChatHistory]): Ð¡Ð¿Ð¸ÑÐ¾Ðº ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Ð¸Ð· Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ….
        
    Returns:
        list[genai_types.Content]: Ð˜ÑÑ‚Ð¾Ñ€Ð¸Ñ Ñ‡Ð°Ñ‚Ð° Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ Gemini API.
    """
    history = []
    for msg in messages:
        history.append(genai_types.Content(role=msg.role, parts=[genai_types.Part.from_text(text=msg.content)]))
    return history


async def process_image_data(image_data: str | None, user_id: int) -> genai_types.Part | None:
    """
    ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð¸ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð¾Ð±ÑŠÐµÐºÑ‚ Part Ð´Ð»Ñ Gemini API.
    
    SECURITY: Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð° Ð”Ðž Ð´ÐµÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð´Ð»Ñ Ð¿Ñ€ÐµÐ´Ð¾Ñ‚Ð²Ñ€Ð°Ñ‰ÐµÐ½Ð¸Ñ DoS Ð°Ñ‚Ð°Ðº Ñ‡ÐµÑ€ÐµÐ· memory exhaustion.
    
    Args:
        image_data (str | None): Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð² Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ base64.
        user_id (int): Ð˜Ð´ÐµÐ½Ñ‚Ð¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ñ€ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ.
        
    Returns:
        genai_types.Part | None: ÐžÐ±ÑŠÐµÐºÑ‚ Part Ñ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸ÐµÐ¼ Ð¸Ð»Ð¸ None, ÐµÑÐ»Ð¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÐµÑ‚ Ð¸Ð»Ð¸ Ð¿Ñ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° Ð¾ÑˆÐ¸Ð±ÐºÐ°.
    """
    MAX_IMAGE_SIZE = MAX_IMAGE_SIZE_MB * 1024 * 1024  # ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ MB Ð² Ð±Ð°Ð¹Ñ‚Ñ‹
    # Base64 ÑƒÐ²ÐµÐ»Ð¸Ñ‡Ð¸Ð²Ð°ÐµÑ‚ Ñ€Ð°Ð·Ð¼ÐµÑ€ Ð½Ð° ~33%, Ð¿Ð¾ÑÑ‚Ð¾Ð¼Ñƒ ÑƒÐ¼Ð½Ð¾Ð¶Ð°ÐµÐ¼ Ð½Ð° 1.4 Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸
    MAX_BASE64_SIZE = MAX_IMAGE_SIZE * 1.4
    
    if not image_data:
        return None
    
    # SECURITY: ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ€Ð°Ð·Ð¼ÐµÑ€ base64 ÑÑ‚Ñ€Ð¾ÐºÐ¸ Ð”Ðž Ð´ÐµÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
    if len(image_data) > MAX_BASE64_SIZE:
        logging.warning(f"Base64 Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ Ð±Ð¾Ð»ÑŒÑˆÐ¾Ðµ ({len(image_data)} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð², Ð¼Ð°ÐºÑÐ¸Ð¼ÑƒÐ¼ {int(MAX_BASE64_SIZE)}) Ð´Ð»Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ {user_id}")
        return None
        
    try:
        image_bytes = base64.b64decode(image_data)
        image_size = len(image_bytes)
        
        # Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð° Ð¿Ð¾ÑÐ»Ðµ Ð´ÐµÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ (double-check)
        if image_size > MAX_IMAGE_SIZE:
            logging.warning(f"Ð˜Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ Ð±Ð¾Ð»ÑŒÑˆÐ¾Ðµ ({image_size} Ð±Ð°Ð¹Ñ‚, Ð¼Ð°ÐºÑÐ¸Ð¼ÑƒÐ¼ {MAX_IMAGE_SIZE} Ð±Ð°Ð¹Ñ‚) Ð´Ð»Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ {user_id}")
            return None
        
        logging.debug(f"ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð¾Ð¼ {image_size} Ð±Ð°Ð¹Ñ‚ Ð´Ð»Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ {user_id}")
        return genai_types.Part.from_bytes(
            data=image_bytes,
            mime_type='image/jpeg'
        )
    except Exception as e:
        logging.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð´Ð»Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ {user_id}: {e}", exc_info=True)
        return None


async def prepare_chat_history(unsummarized_messages: list[ChatHistory], formatted_message: str, image_part: genai_types.Part | None, is_premium: bool = False) -> list[genai_types.Content]:
    """
    ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÑ‚ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ Ñ‡Ð°Ñ‚Ð° Ð´Ð»Ñ Gemini API, Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ðµ Ð¿Ð¾ Ð»Ð¸Ð¼Ð¸Ñ‚Ñƒ Ð¸ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ³Ð¾ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ.

    Args:
        unsummarized_messages (list[ChatHistory]): ÐÐµÑÑƒÐ¼Ð¼Ð°Ñ€Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ Ð¸Ð· Ð‘Ð”.
        formatted_message (str): ÐžÑ‚Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ.
        image_part (genai_types.Part | None): Ð§Ð°ÑÑ‚ÑŒ Ñ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸ÐµÐ¼, ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ.
        is_premium (bool): Ð¯Ð²Ð»ÑÐµÑ‚ÑÑ Ð»Ð¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ premium (Ð´Ð»Ñ Ñ€Ð°Ð·Ð½Ñ‹Ñ… Ð»Ð¸Ð¼Ð¸Ñ‚Ð¾Ð² Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸).

    Returns:
        list[genai_types.Content]: Ð“Ð¾Ñ‚Ð¾Ð²Ð°Ñ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ñ Ñ‡Ð°Ñ‚Ð°.
    """
    # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ñ€Ð°Ð·Ð½Ñ‹Ðµ Ð»Ð¸Ð¼Ð¸Ñ‚Ñ‹ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸ Ð´Ð»Ñ FREE Ð¸ PREMIUM Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¹
    history_limit = CHAT_HISTORY_LIMIT_PREMIUM if is_premium else CHAT_HISTORY_LIMIT_FREE
    history = create_history_from_messages(unsummarized_messages[-history_limit:])

    user_parts = [genai_types.Part.from_text(text=formatted_message)]
    if image_part:
        user_parts.insert(0, image_part)

    history.append(genai_types.Content(role='user', parts=user_parts))
    return history


async def manage_function_calls(response: Any, history: list[genai_types.Content], available_functions: dict[str, Any], user_id: int) -> str | None:
    """
    ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ Ð²Ñ‹Ð·Ð¾Ð²Ñ‹ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¹ Ð¼Ð¾Ð´ÐµÐ»ÑŒÑŽ Gemini.
    
    Args:
        response: ÐžÑ‚Ð²ÐµÑ‚ Ð¾Ñ‚ Gemini API.
        history (list[genai_types.Content]): Ð¢ÐµÐºÑƒÑ‰Ð°Ñ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ñ Ñ‡Ð°Ñ‚Ð°.
        available_functions (dict[str, Any]): Ð”Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸.
        user_id (int): ID Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ.
        
    Returns:
        str | None: Base64 image data if generate_image was called, else None.
    """
    if not response.function_calls:
        return None
    
    function_call = response.function_calls[0]
    function_name = function_call.name
    logging.debug(f"ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð²Ñ‹Ð·Ð²Ð°Ð»Ð° Ñ„ÑƒÐ½ÐºÑ†Ð¸ÑŽ: {function_name}")

    if function_name not in available_functions:
        logging.warning(f"ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð¿Ð¾Ð¿Ñ‹Ñ‚Ð°Ð»Ð°ÑÑŒ Ð²Ñ‹Ð·Ð²Ð°Ñ‚ÑŒ Ð½ÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½ÑƒÑŽ Ñ„ÑƒÐ½ÐºÑ†Ð¸ÑŽ '{function_name}'")
        history.append(genai_types.Content(role="model", parts=[genai_types.Part.from_text(text=f"Ð’Ñ‹Ð·Ð²Ð°Ð½Ð° Ð½ÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ð°Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ: {function_name}")]))
        return None

    function_to_call = available_functions[function_name]
    function_args = dict(function_call.args)
    logging.debug(f"ÐÑ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸: {function_args}")

    function_response_data = await function_to_call(**function_args)
    logging.debug(f"Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ '{function_name}': {function_response_data if function_name != 'generate_image' else 'Image generated'}")

    history.append(response.candidates[0].content)
    history.append(genai_types.Content(
        role="function",
        parts=[genai_types.Part(
            function_response=genai_types.FunctionResponse(
                name=function_name,
                response={"result": function_response_data},
            )
        )]
    ))
    
    if function_name == "generate_image":
        return function_response_data
    return None


async def handle_final_response(response: Any, user_id: int, candidate: Any) -> str:
    """
    ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ Ð¾Ñ‚ Ð¼Ð¾Ð´ÐµÐ»Ð¸, Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ fallback'Ñ‹ Ð´Ð»Ñ Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²Ð¸Ñ Ñ‚ÐµÐºÑÑ‚Ð°.
    
    Args:
        response: ÐžÑ‚Ð²ÐµÑ‚ Ð¾Ñ‚ Gemini API.
        user_id (int): ID Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ.
        candidate: ÐšÐ°Ð½Ð´Ð¸Ð´Ð°Ñ‚ Ð¾Ñ‚Ð²ÐµÑ‚Ð°.
        
    Returns:
        str: Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚ Ð¾Ñ‚Ð²ÐµÑ‚Ð°.
    """
    final_response = ""
    if response.text:
        final_response = response.text.strip()
    else:
        finish_reason = candidate.finish_reason.name
        logging.warning(f"ÐžÑ‚Ð²ÐµÑ‚ Ð¾Ñ‚ API Ð´Ð»Ñ {user_id} Ð½Ðµ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ Ñ‚ÐµÐºÑÑ‚Ð°. ÐŸÑ€Ð¸Ñ‡Ð¸Ð½Ð°: {finish_reason}")
        if finish_reason == 'MAX_TOKENS':
            final_response = "ÐžÐ¹, Ñ Ñ‚Ð°Ðº ÑƒÐ²Ð»ÐµÐºÐ»Ð°ÑÑŒ, Ñ‡Ñ‚Ð¾ Ð¼Ñ‹ÑÐ»ÑŒ Ð½Ðµ Ð¿Ð¾Ð¼ÐµÑÑ‚Ð¸Ð»Ð°ÑÑŒ Ð² Ð¾Ð´Ð½Ð¾ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ. Ð¡Ð¿Ñ€Ð¾ÑÐ¸ ÐµÑ‰Ðµ Ñ€Ð°Ð·, Ñ Ð¿Ð¾Ð¿Ñ€Ð¾Ð±ÑƒÑŽ Ð¾Ñ‚Ð²ÐµÑ‚Ð¸Ñ‚ÑŒ ÐºÐ¾Ñ€Ð¾Ñ‡Ðµ."
        elif finish_reason == 'SAFETY':
            final_response = "Ð¯ Ð½Ðµ Ð¼Ð¾Ð³Ñƒ Ð¾Ð±ÑÑƒÐ¶Ð´Ð°Ñ‚ÑŒ ÑÑ‚Ñƒ Ñ‚ÐµÐ¼Ñƒ, Ð¿Ñ€Ð¾ÑÑ‚Ð¸. Ð”Ð°Ð²Ð°Ð¹ ÑÐ¼ÐµÐ½Ð¸Ð¼ Ñ‚ÐµÐ¼Ñƒ?"
        else:
            final_response = "Ð¯ Ð½Ðµ Ð¼Ð¾Ð³Ñƒ ÑÐµÐ¹Ñ‡Ð°Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð¸Ñ‚ÑŒ. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹ Ð¿ÐµÑ€ÐµÑ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ."
    
    return final_response


async def generate_ai_response(user_id: int, user_message: str | None, timestamp: datetime, image_data: str | None = None) -> dict[str, str | None]:
    """
    Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ Ð¾Ñ‚Ð²ÐµÑ‚ AI Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ `generate_content`, ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÑ Ð¸ Ð¸Ð·Ð²Ð»ÐµÐºÐ°Ñ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ Ñ‡Ð°Ñ‚Ð° Ð¸Ð· Ð‘Ð”.
    ÐŸÐ¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÑƒ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹.

    Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ ÐºÐ»Ð°ÑÑ AIResponseGenerator Ð´Ð»Ñ Ð»ÑƒÑ‡ÑˆÐµÐ¹ Ð¾Ñ€Ð³Ð°Ð½Ð¸Ð·Ð°Ñ†Ð¸Ð¸ ÐºÐ¾Ð´Ð°.

    Args:
        user_id: ID Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
        user_message: Ð¡Ð¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ (Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ None Ð´Ð»Ñ image-only ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹)
        timestamp: Ð’Ñ€ÐµÐ¼ÐµÐ½Ð½Ð°Ñ Ð¼ÐµÑ‚ÐºÐ° ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ
        image_data: ÐžÐ¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ Ð² base64

    Returns:
        Dict Ñ ÐºÐ»ÑŽÑ‡Ð°Ð¼Ð¸ 'text' Ð¸ 'image_base64'
    """
    # ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ ÑÐ»ÑƒÑ‡Ð°Ð¹ ÐºÐ¾Ð³Ð´Ð° Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¾ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ð±ÐµÐ· Ñ‚ÐµÐºÑÑ‚Ð°
    if not user_message and image_data:
        user_message = "[Ð˜Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ]"
        logging.info(f"ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¾ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ Ð¾Ñ‚ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ {user_id} Ð² {timestamp} Ð±ÐµÐ· Ñ‚ÐµÐºÑÑ‚Ð°")
    else:
        logging.info(f"ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¾ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð¾Ñ‚ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ {user_id} Ð² {timestamp}: '{user_message}'")

    generator = AIResponseGenerator(user_id, user_message, timestamp, image_data)
    return await generator.generate()

@gemini_circuit_breaker.call
@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10), # Ð­ÐºÑÐ¿Ð¾Ð½ÐµÐ½Ñ†Ð¸Ð°Ð»ÑŒÐ½Ð°Ñ Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÐ°
    retry=retry_if_exception_type(APIError),
    reraise=True
)
async def call_gemini_api_with_retry(user_id: int, model_name: str, contents: list, tools: list, system_instruction: str, thinking_budget: int = 0):
    """
    Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ Ð²Ñ‹Ð·Ð¾Ð² Ðº Gemini API Ñ Ð»Ð¾Ð³Ð¸ÐºÐ¾Ð¹ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ð½Ñ‹Ñ… Ð¿Ð¾Ð¿Ñ‹Ñ‚Ð¾Ðº Ð¸ Circuit Breaker Ð·Ð°Ñ‰Ð¸Ñ‚Ð¾Ð¹.
    
    Circuit Breaker Ð·Ð°Ñ‰Ð¸Ñ‰Ð°ÐµÑ‚ Ð¾Ñ‚ ÐºÐ°ÑÐºÐ°Ð´Ð½Ñ‹Ñ… ÑÐ±Ð¾ÐµÐ²:
    - Ð‘Ð»Ð¾ÐºÐ¸Ñ€ÑƒÐµÑ‚ Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹ Ð¿Ð¾ÑÐ»Ðµ 5 ÑÐ±Ð¾ÐµÐ² Ð¿Ð¾Ð´Ñ€ÑÐ´
    - Ð–Ð´ÐµÑ‚ 60 ÑÐµÐºÑƒÐ½Ð´ Ð¿ÐµÑ€ÐµÐ´ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ð½Ð¾Ð¹ Ð¿Ð¾Ð¿Ñ‹Ñ‚ÐºÐ¾Ð¹
    - Ð’Ð¾ÑÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÑ‚ÑÑ Ð¿Ð¾ÑÐ»Ðµ 2 ÑƒÑÐ¿ÐµÑˆÐ½Ñ‹Ñ… Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð²
    
    Args:
        user_id (int): Ð£Ð½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¸Ð´ÐµÐ½Ñ‚Ð¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ñ€ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ.
        model_name (str): ÐÐ°Ð·Ð²Ð°Ð½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð´Ð»Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸.
        contents (list): Ð¡Ð¾Ð´ÐµÑ€Ð¶Ð°Ð½Ð¸Ðµ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°.
        tools (list): Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ð´Ð»Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒÑŽ.
        system_instruction (str): Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð½Ð°Ñ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ñ Ð´Ð»Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸.
        thinking_budget (int): Ð‘ÑŽÐ´Ð¶ÐµÑ‚ Ð´Ð»Ñ "Ð¼Ñ‹ÑˆÐ»ÐµÐ½Ð¸Ñ" Ð¼Ð¾Ð´ÐµÐ»Ð¸.
        
    Returns:
        response: ÐžÑ‚Ð²ÐµÑ‚ Ð¾Ñ‚ API Gemini.
        
    Raises:
        CircuitBreakerError: Ð•ÑÐ»Ð¸ circuit breaker Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚
        APIError: ÐŸÑ€Ð¸ Ð¾ÑˆÐ¸Ð±ÐºÐ°Ñ… API Ð¿Ð¾ÑÐ»Ðµ retry
    """
    logging.debug(f"ÐŸÐ¾Ð¿Ñ‹Ñ‚ÐºÐ° Ð²Ñ‹Ð·Ð¾Ð²Ð° Gemini API Ð´Ð»Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ {user_id}")
    
    # Log system instruction and context for debugging
    system_log = system_instruction[:500] + "..." if len(system_instruction) > 500 else system_instruction
    logging.debug(f"Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð½Ð°Ñ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ñ Ð´Ð»Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ {user_id}: {system_log}")
    
    context_parts = []
    for content in contents:
        role = content.role
        text_parts = [part.text for part in content.parts if hasattr(part, 'text') and part.text]
        if text_parts:
            text = " ".join(text_parts)[:200] + "..." if len(" ".join(text_parts)) > 200 else " ".join(text_parts)
            context_parts.append(f"{role}: {text}")
        else:
            context_parts.append(f"{role}: [no text, possibly image]")
    context_str = "\n".join(context_parts)
    logging.debug(f"ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚, Ð¿ÐµÑ€ÐµÐ´Ð°Ð½Ð½Ñ‹Ð¹ Ð² Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ {user_id}:\n{context_str}")
    
    try:
        response = await client.aio.models.generate_content(
            model=model_name,
            contents=contents,
            config=genai_types.GenerateContentConfig(
                tools=tools,
                system_instruction=system_instruction,
                thinking_config=genai_types.ThinkingConfig(
                    thinking_budget=thinking_budget
                )
            )
        )
        
        # Log token usage for debugging
        if hasattr(response, 'usage_metadata') and response.usage_metadata:
            logging.debug(f"ÐŸÐ¾Ñ‚Ñ€ÐµÐ±Ð»ÐµÐ½Ð¸Ðµ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð´Ð»Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ {user_id}: prompt={response.usage_metadata.prompt_token_count}, candidates={response.usage_metadata.candidates_token_count}")
        
        return response
    except APIError as e:
        logging.warning(f"ÐžÑˆÐ¸Ð±ÐºÐ° Gemini API Ð´Ð»Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ {user_id}: {e}. ÐŸÐ¾Ð²Ñ‚Ð¾Ñ€Ð½Ð°Ñ Ð¿Ð¾Ð¿Ñ‹Ñ‚ÐºÐ°...")
        raise
    except Exception as e:
        logging.error(f"ÐÐµÐ¿Ñ€ÐµÐ´Ð²Ð¸Ð´ÐµÐ½Ð½Ð°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð²Ñ‹Ð·Ð¾Ð²Ðµ Gemini API Ð´Ð»Ñ {user_id}: {e}", exc_info=True)
        raise


async def generate_image(prompt: str) -> str:
    """
    Generates an image using the Gemini preview model and returns base64 encoded data.
    
    Args:
        prompt (str): Text description for the image.
        
    Returns:
        str: Base64 encoded image data.
    """
    try:
        response = await client.aio.models.generate_content(
            model="gemini-2.5-flash-image-preview",
            contents=[prompt],
        )
        
        if response.candidates:
            for part in response.candidates[0].content.parts:
                if part.inline_data is not None:
                    image_data = part.inline_data.data
                    image_b64 = base64.b64encode(image_data).decode('utf-8')
                    logging.debug(f"Image generated successfully for prompt: {prompt[:50]}...")
                    return image_b64
        
        raise ValueError("Image generation failed: No image data in response")
    except Exception as e:
        logging.error(f"Error generating image: {e}")
        raise
